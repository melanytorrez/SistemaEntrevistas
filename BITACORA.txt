BITACORA DE DESARROLLO - SISTEMA DE ANALISIS DE ENTREVISTAS

======================================================================
DIA 0: PREPARACION Y DATOS
Fecha: 14/12/2025
======================================================================

1. GENERACION DE DATASET
   - Actividad: Grabacion de 3 videos de validacion (30-60 segundos).
   - Escenarios cubiertos:
     a) Video 1(49): Cambio emocional brusco (Positivo a Negativo).
     b) Video 2(14): Incongruencia (Expresion facial vs. Contenido del texto).
     c) Video 3(12): Linea base neutral (Lectura tecnica).
   - Ground Truth: Creacion del archivo 'validacion_dataset.xlsx' con etiquetado manual de emociones y tiempos para calculo posterior de precision.

2. PREPARACION DEL ENTORNO
   - Estructura de directorios creada (/data, /src, /models, /output).
   - Instalacion de dependencias base: OpenCV, DeepFace, Whisper, Torch, Transformers.

======================================================================
DIA 1: CONFIGURACION, PROTOTIPADO Y DISEO
Fecha: 15/12/2025
ESTADO: COMPLETADO
======================================================================

1. RESOLUCION DE INCIDENCIAS TECNICAS
   - Incidencia: Conflicto de dependencias entre TensorFlow 2.x y Keras.
     Solucion: Instalacion de paquete 'tf-keras' para compatibilidad con RetinaFace.
   - Incidencia: Whisper fallaba por ausencia de binarios de FFmpeg en Windows.
     Solucion: Instalacion via Winget y actualizacion de variables de entorno (PATH).
   - Incidencia: Deprecacion de metodos en MoviePy 2.0.
     Solucion: Refactorizacion de codigo reemplazando .subclip() por .subclipped().

2. PRUEBAS UNITARIAS (SMOKE TESTS)
   - Modulo de Vision (DeepFace):
     Script: src/test_vision_day1.py
     Resultado: Descarga exitosa de pesos VGG-Face. Inferencia correcta en frame estatico (Confianza > 90%).
   - Modulo de Audio (Whisper):
     Script: src/test_audio_day1.py
     Resultado: Extraccion de audio exitosa y transcripcion correcta utilizando modelo 'base' en CPU.

3. INVESTIGACION DE MODELOS NLP
   - Se evaluaron opciones para Analisis de Sentimiento.
   - Seleccion: Modelo 'j-hartmann/emotion-english-distilroberta-base'.
   - Justificacion: Clasifica texto en las mismas 7 emociones universales (Ekman) que utiliza DeepFace, facilitando la integracion directa sin mapeos complejos.
   - Prueba: Script src/test_nlp_day1.py ejecutado exitosamente.

4. REUNION DE CIERRE Y DISEO
   - Se definio la Arquitectura de Datos (ver archivo DISENO_ARQUITECTURA.md).
   - Se establecio el formato JSON como interfaz de comunicacion entre modulos.
   - Planificacion Dia 2: Desarrollo de bucles de procesamiento completo para Video y Audio.

======================================================================
DIA 2: DESARROLLO PARALELO DE MODULOS
Fecha: 16/12/2025
ESTADO: COMPLETADO
======================================================================

1. IMPLEMENTACION DE INFRAESTRUCTURA COMUN (MODULO UTILIDADES)
   - Objetivo: Crear funciones transversales (Helpers) para estandarizar Logs, Rutas y Formatos de Salida.
   - Archivo base: src/utils.py
   - Funcionalidades implementadas:
     a) Sistema de Logging Centralizado (setup_logger): Salida dual consola/archivo.
     b) Gesti贸n de Rutas (get_video_path): Abstracci贸n para evitar errores de rutas.
     c) Estandarizaci贸n de Salida (save_analysis_json): Wrapper para garantizar estructura JSON consistente.

2. PRUEBAS DE INTEGRACION DE UTILIDADES
   - Script de prueba: test_utilities_module()
   - Resultados: Log generado correctamente y JSON de prueba validado en /output.

3. IMPLEMENTACION DEL MODULO DE VISION (CNN)
   - Archivo: src/vision_module_day2.py
   - Funcionalidad: Procesamiento frame a frame con sampling (1 fps).
   - Tecnolog铆a: DeepFace (OpenCV backend).
   - Salida: JSON en /results/vision_analysis.json.
   - Incidencias resueltas: Serializaci贸n de tipos numpy (float32) a JSON nativo.

4. IMPLEMENTACION DEL MODULO DE AUDIO (ASR + NLP)
   - Archivo: src/audio_module.py
   - Enfoque: Procesamiento en cascada (Pipeline) Video -> Audio -> Texto -> Emoci贸n.
   - Tecnolog铆as Implementadas:
     a) OpenAI Whisper (modelo 'base'): Transcripci贸n robusta con timestamps precisos.
     b) BERT Multilingual (bhadresh-savani): An谩lisis de sentimiento con soporte nativo para Espa帽ol.
   - Incidencias T茅cnicas Resueltas:
     a) Compatibilidad MoviePy 2.0: Se elimin贸 el argumento 'verbose' (deprecado) que causaba excepciones cr铆ticas en la extracci贸n de audio.
     b) Adaptaci贸n Ling眉铆stica: El modelo inicial (ingl茅s) generaba falsos negativos ("neutral") en frases en espa帽ol. Se migr贸 a 'bert-base-multilingual-uncased-emotion' logrando discriminaci贸n correcta de polaridad.
   - Resultados de Validaci贸n:
     - Script: src/test_audio_module_day2.py
     - Salida: JSON generado en /results/audio_analysis_video1.json.
     - Precisi贸n: Se verific贸 la detecci贸n de 'joy' (alegr铆a, score 0.89) y 'anger' (ira, score 0.94) coincidiendo con el contenido sem谩ntico del video de prueba.
4. INTEGRACIN MULTIMODAL (Da 3)
   - Archivo: src/integration_module_day3.py
   - Objetivo: Fusin de streams de Audio y Visin.
   - Lgica implementada:
     a) Carga de JSONs estandarizados (Vision y Audio).
     b) Bsqueda temporal: Para cada segmento de audio [t_start, t_end], recupera frames coincidentes.
     c) Algoritmo de decisin: Promedio de emociones visuales vs Emocin del texto.
     d) Deteccin de Incongruencias: Matriz de opuestos (ej. Texto Triste + Cara Feliz = Incongruencia).
   - Resultado: Reporte integrado generado en /results/integrated_report_video1.json.
