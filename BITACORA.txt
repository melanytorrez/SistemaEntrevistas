BITACORA DE DESARROLLO - SISTEMA DE ANALISIS DE ENTREVISTAS

======================================================================
DIA 0: PREPARACION Y DATOS
Fecha: 14/12/2025
======================================================================

1. GENERACION DE DATASET
   - Actividad: Grabacion de 3 videos de validacion (30-60 segundos).
   - Escenarios cubiertos:
     a) Video 1(49): Cambio emocional brusco (Positivo a Negativo).
     b) Video 2(14): Incongruencia (Expresion facial vs. Contenido del texto).
     c) Video 3(12): Linea base neutral (Lectura tecnica).
   - Ground Truth: Creacion del archivo 'validacion_dataset.xlsx' con etiquetado manual de emociones y tiempos para calculo posterior de precision.

2. PREPARACION DEL ENTORNO
   - Estructura de directorios creada (/data, /src, /models, /output).
   - Instalacion de dependencias base: OpenCV, DeepFace, Whisper, Torch, Transformers.

======================================================================
DIA 1: CONFIGURACION, PROTOTIPADO Y DISEÑO
Fecha: 15/12/2025
ESTADO: COMPLETADO
======================================================================

1. RESOLUCION DE INCIDENCIAS TECNICAS
   - Incidencia: Conflicto de dependencias entre TensorFlow 2.x y Keras.
     Solucion: Instalacion de paquete 'tf-keras' para compatibilidad con RetinaFace.
   - Incidencia: Whisper fallaba por ausencia de binarios de FFmpeg en Windows.
     Solucion: Instalacion via Winget y actualizacion de variables de entorno (PATH).
   - Incidencia: Deprecacion de metodos en MoviePy 2.0.
     Solucion: Refactorizacion de codigo reemplazando .subclip() por .subclipped().

2. PRUEBAS UNITARIAS (SMOKE TESTS)
   - Modulo de Vision (DeepFace):
     Script: src/test_vision_day1.py
     Resultado: Descarga exitosa de pesos VGG-Face. Inferencia correcta en frame estatico (Confianza > 90%).
   - Modulo de Audio (Whisper):
     Script: src/test_audio_day1.py
     Resultado: Extraccion de audio exitosa y transcripcion correcta utilizando modelo 'base' en CPU.

3. INVESTIGACION DE MODELOS NLP
   - Se evaluaron opciones para Analisis de Sentimiento.
   - Seleccion: Modelo 'j-hartmann/emotion-english-distilroberta-base'.
   - Justificacion: Clasifica texto en las mismas 7 emociones universales (Ekman) que utiliza DeepFace, facilitando la integracion directa sin mapeos complejos.
   - Prueba: Script src/test_nlp_day1.py ejecutado exitosamente.

4. REUNION DE CIERRE Y DISEÑO
   - Se definio la Arquitectura de Datos (ver archivo DISENO_ARQUITECTURA.md).
   - Se establecio el formato JSON como interfaz de comunicacion entre modulos.
   - Planificacion Dia 2: Desarrollo de bucles de procesamiento completo para Video y Audio.

======================================================================
DIA 2: DESARROLLO PARALELO DE MODULOS
Fecha: 16/12/2025
ESTADO: COMPLETADO
======================================================================

1. IMPLEMENTACION DE INFRAESTRUCTURA COMUN (MODULO UTILIDADES)
   - Objetivo: Crear funciones transversales (Helpers) para estandarizar Logs, Rutas y Formatos de Salida.
   - Archivo base: src/utils.py
   - Funcionalidades implementadas:
     a) Sistema de Logging Centralizado (setup_logger): Salida dual consola/archivo.
     b) Gestión de Rutas (get_video_path): Abstracción para evitar errores de rutas.
     c) Estandarización de Salida (save_analysis_json): Wrapper para garantizar estructura JSON consistente.

2. PRUEBAS DE INTEGRACION DE UTILIDADES
   - Script de prueba: test_utilities_module()
   - Resultados: Log generado correctamente y JSON de prueba validado en /output.

3. IMPLEMENTACION DEL MODULO DE VISION (CNN)
   - Archivo: src/vision_module_day2.py
   - Funcionalidad: Procesamiento frame a frame con sampling (1 fps).
   - Tecnología: DeepFace (OpenCV backend).
   - Salida: JSON en /results/vision_analysis.json.
   - Incidencias resueltas: Serialización de tipos numpy (float32) a JSON nativo.

4. IMPLEMENTACION DEL MODULO DE AUDIO (ASR + NLP)
   - Archivo: src/audio_module.py
   - Enfoque: Procesamiento en cascada (Pipeline) Video -> Audio -> Texto -> Emoción.
   - Tecnologías Implementadas:
     a) OpenAI Whisper (modelo 'base'): Transcripción robusta con timestamps precisos.
     b) BERT Multilingual (bhadresh-savani): Análisis de sentimiento con soporte nativo para Español.
   - Incidencias Técnicas Resueltas:
     a) Compatibilidad MoviePy 2.0: Se eliminó el argumento 'verbose' (deprecado) que causaba excepciones críticas en la extracción de audio.
     b) Adaptación Lingüística: El modelo inicial (inglés) generaba falsos negativos ("neutral") en frases en español. Se migró a 'bert-base-multilingual-uncased-emotion' logrando discriminación correcta de polaridad.
   - Resultados de Validación:
     - Script: src/test_audio_module_day2.py
     - Salida: JSON generado en /results/audio_analysis_video1.json.
     - Precisión: Se verificó la detección de 'joy' (alegría, score 0.89) y 'anger' (ira, score 0.94) coincidiendo con el contenido semántico del video de prueba.
     
======================================================================
DIA 3: INTEGRACION Y SINCRONIZACION
Fecha: 17/12/2025
ESTADO: COMPLETADO
======================================================================

1. IMPLEMENTACION DE LOGICA DE FUSION (PAREJAS 1 & 2)
  - Archivo: src/integration_module_day3.py
  - Objetivo: Fusión de streams de Audio y Visión (Sincronización y Merge).
  - Lógica implementada:
    a) Carga de JSONs estandarizados de las etapas previas.
    b) Búsqueda Temporal: Algoritmo que recupera frames visuales dentro del intervalo [t_start, t_end] de cada frase de audio.
    c) Algoritmo de Decisión: Comparación entre la emoción del texto (NLP) y la moda estadística de las emociones faciales en ese lapso.
    d) Matriz de Incongruencia: Implementación de reglas lógicas para detectar conflictos (ej. Texto 'Triste' + Cara 'Feliz' = Incongruencia).
  - Resultado: Generación exitosa del reporte integrado en /results/integrated_report_video1.json.

2. VISUALIZACION DE DATOS INTEGRADOS (PAREJA 3)
  - Archivo: src/visualizer.py
  - Objetivo: Generar representación gráfica comprensible para el usuario final (Visualización Integrada).
  - Implementación Técnica:
    a) Uso de librería Matplotlib para graficar series temporales multimodales.
    b) Mapeo de Polaridad: Conversión de etiquetas discretas (happy, sad, neutral) a valores numéricos (+1, 0, -1) para representación en Eje Y.
    c) Indicador Visual de Congruencia: Uso de zonas de fondo coloreadas (Verde = Coherente / Rojo = Incongruente) basadas en el análisis lógico del paso anterior.
  - Entregable Final: Imagen 'final_integration_chart.png' generada correctamente, evidenciando la evolución temporal de ambas modalidades y las alertas de incongruencia.